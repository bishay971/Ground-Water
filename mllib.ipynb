{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml import Pipeline  \n",
    "from pyspark.ml.feature import StringIndexer, Imputer, StandardScaler, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"App\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "filePath_2018 = \"\"\"C:/Users/bishe/OneDrive/Desktop/Personal/Softwerica/Big data/ground_water_quality_2018_post.csv\"\"\"\n",
    "filePath_2019 = \"\"\"C:/Users/bishe/OneDrive/Desktop/Personal/Softwerica/Big data/ground_water_quality_2019_post.csv\"\"\"\n",
    "filePath_2020 = \"\"\"C:/Users/bishe/OneDrive/Desktop/Personal/Softwerica/Big data/ground_water_quality_2020_post.csv\"\"\"\n",
    "\n",
    "gw_2018 = spark.read.csv(filePath_2018, header=True)\n",
    "gw_2019 = spark.read.csv(filePath_2019, header=True)\n",
    "gw_2020 = spark.read.csv(filePath_2020, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sno', 'district', 'mandal', 'village', 'lat_gis', 'long_gis', 'gwl', 'season', 'pH', 'E.C', 'TDS', 'CO3', 'HCO3', 'Cl', 'F', 'NO3 ', 'SO4', 'Na', 'K', 'Ca', 'Mg', 'T.H', 'SAR', 'Classification', 'RSC  meq  / L', 'Classification.1']\n",
      "['sno', 'district', 'mandal', 'village', 'lat_gis', 'long_gis', 'gwl', 'season', 'pH', 'EC', 'TDS', 'CO_-2 ', 'HCO_ - ', 'Cl -', 'F -', 'NO3- ', 'SO4-2', 'Na+', 'K+', 'Ca+2', 'Mg+2', 'T.H', 'SAR', 'Classification', 'RSC  meq  / L', 'Classification.1']\n",
      "['sno', 'district', 'mandal', 'village', 'lat_gis', 'long_gis', 'gwl', 'season', 'pH', 'E.C', 'TDS', 'CO3', 'HCO3', 'Cl', 'F', 'NO3 ', 'SO4', 'Na', 'K', 'Ca', 'Mg', 'T.H', 'SAR', 'Classification', 'RSC  meq  / L', 'Classification.1']\n"
     ]
    }
   ],
   "source": [
    "# Show the column names\n",
    "print(gw_2018.columns)\n",
    "print(gw_2019.columns)\n",
    "print(gw_2020.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sno: string (nullable = true)\n",
      " |-- district: string (nullable = true)\n",
      " |-- mandal: string (nullable = true)\n",
      " |-- village: string (nullable = true)\n",
      " |-- lat_gis: string (nullable = true)\n",
      " |-- long_gis: string (nullable = true)\n",
      " |-- gwl: string (nullable = true)\n",
      " |-- season: string (nullable = true)\n",
      " |-- pH: string (nullable = true)\n",
      " |-- E.C: string (nullable = true)\n",
      " |-- TDS: string (nullable = true)\n",
      " |-- CO3: string (nullable = true)\n",
      " |-- HCO3: string (nullable = true)\n",
      " |-- Cl: string (nullable = true)\n",
      " |-- F: string (nullable = true)\n",
      " |-- NO3 : string (nullable = true)\n",
      " |-- SO4: string (nullable = true)\n",
      " |-- Na: string (nullable = true)\n",
      " |-- K: string (nullable = true)\n",
      " |-- Ca: string (nullable = true)\n",
      " |-- Mg: string (nullable = true)\n",
      " |-- T.H: string (nullable = true)\n",
      " |-- SAR: string (nullable = true)\n",
      " |-- Classification: string (nullable = true)\n",
      " |-- RSC  meq  / L: string (nullable = true)\n",
      " |-- Classification.1: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gw_2018.printSchema()\n",
    "#gw_2019.printSchema()\n",
    "#gw_2020.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gw_2018.show(n=34)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assigning same column names for all dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['sno', 'district', 'mandal', 'village', 'lat_gis', 'long_gis', 'gwl', 'season', 'pH', 'EC', 'TDS', \n",
    "                'CO3', 'HCO3', 'Cl', 'F', 'NO3 ', 'SO4', 'Na', 'K', 'Ca', 'Mg', 'TH', 'SAR', \n",
    "                'Classification', 'RSC  meq  / L', 'Classification1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame with the specified column names\n",
    "gw_2018 = gw_2018.toDF(*column_names).na.drop()\n",
    "gw_2019 = gw_2019.toDF(*column_names).na.drop()\n",
    "gw_2020= gw_2020.toDF(*column_names).na.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DELETE CLASSES THAT ARE NOT PRESENT IN TESTING SET\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|Classification|\n",
      "+--------------+\n",
      "|          C1S1|\n",
      "|          C2S2|\n",
      "|          C3S2|\n",
      "|          C4S4|\n",
      "|          C2S1|\n",
      "|          C4S1|\n",
      "|          C4S2|\n",
      "|          C4S3|\n",
      "|          C3S3|\n",
      "|          C3S4|\n",
      "|          C3S1|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unique_names = gw_2018.select(\"Classification\").distinct()\n",
    "unique_names.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|Classification|\n",
      "+--------------+\n",
      "|          C1S1|\n",
      "|          C3S2|\n",
      "|          C2S1|\n",
      "|          C4S1|\n",
      "|          C4S2|\n",
      "|          C4S3|\n",
      "|            OG|\n",
      "|          C3S1|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unique_names = gw_2019.select(\"Classification\").distinct()\n",
    "unique_names.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "# Replace 'O.G.' with 'OG' in the 'Classification' column\n",
    "gw_2020 = gw_2020.withColumn(\"Classification\", \n",
    "                   when(col(\"Classification\") == \"O.G\", \"OG\")\n",
    "                   .otherwise(col(\"Classification\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|Classification|\n",
      "+--------------+\n",
      "|          C3S2|\n",
      "|          C4S4|\n",
      "|          C2S1|\n",
      "|          C4S1|\n",
      "|          C4S2|\n",
      "|          C4S3|\n",
      "|          C3S3|\n",
      "|          C3S1|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unique_names = gw_2020.select(\"Classification\").distinct()\n",
    "unique_names.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMBINE 2018, 2019 AND 2020 DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw_combined = gw_2018.union(gw_2019)\n",
    "combined_df = gw_combined.union(gw_2020)\n",
    "combined_df = combined_df.filter(combined_df['Classification'] != 'OG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_csv(df, path):\n",
    "    pandas_df = df.toPandas()\n",
    "    pandas_df.to_csv(path, index=False)\n",
    "\n",
    "\n",
    "# Usage\n",
    "export_to_csv(combined_df, \"C:/hadoop/combined_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SEPERATE FEATURES AND CLASSIFICATION AND CAST COLUMNS TO FLOAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'Classification'\n",
    "feature_columns = ['gwl', 'pH', 'EC', 'TDS','CO3', 'HCO3', 'Cl', 'F', 'NO3 ', 'SO4',\n",
    "            'Na', 'K', 'Ca', 'Mg', 'TH', 'SAR', 'RSC  meq  / L']\n",
    "all_columns = ['Classification','gwl', 'pH', 'EC', 'TDS',\n",
    "            'CO3', 'HCO3', 'Cl', 'F', 'NO3 ', 'SO4', 'Na', 'K', 'Ca', 'Mg', 'TH', 'SAR', 'RSC  meq  / L']\n",
    "\n",
    "# Cast columns to float\n",
    "for column in feature_columns:\n",
    "    combined_df = combined_df.withColumn(column, col(column).cast(\"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = combined_df[all_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXPORT AS CSV FOR TABLEAU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Handle missing values by imputing with mean for numerical columns\n",
    "imputer = Imputer(inputCols=feature_columns, outputCols=[f\"{col}_imputed\" for col in feature_columns])\n",
    "\n",
    "# Encode target variable\n",
    "target_indexer = StringIndexer(inputCol=target_column, outputCol=target_column + '_indexed')\n",
    "\n",
    "# Combine all features into a single vector\n",
    "assembler = VectorAssembler(inputCols=[f\"{col}_imputed\" for col in feature_columns], outputCol=\"features\")\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline(stages=[imputer, target_indexer, assembler, scaler])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the pipeline\n",
    "pipeline_model = pipeline.fit(combined_data)\n",
    "transformed_data = pipeline_model.transform(combined_data)\n",
    "\n",
    "# Select features and target variable\n",
    "final_data = transformed_data.select(\"scaledFeatures\", target_column + '_indexed',target_column)\n",
    "final_data = final_data.withColumnRenamed(\"scaledFeatures\", \"features\").withColumnRenamed(target_column + '_indexed', \"label\")\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_data, val_data = final_data.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9408284023668639\n",
      "+--------------------+-----+----------+--------------+\n",
      "|            features|label|prediction|Classification|\n",
      "+--------------------+-----+----------+--------------+\n",
      "|[0.13858473032769...|  0.0|       0.0|          C3S1|\n",
      "|[0.21777601154375...|  0.0|       0.0|          C3S1|\n",
      "|[0.23229441441119...|  1.0|       1.0|          C2S1|\n",
      "|[0.27980916740693...|  0.0|       0.0|          C3S1|\n",
      "|[0.33260336537357...|  1.0|       1.0|          C2S1|\n",
      "|[0.34316219237978...|  0.0|       0.0|          C3S1|\n",
      "|[0.36296002448422...|  0.0|       0.0|          C3S1|\n",
      "|[0.40255565722530...|  1.0|       1.0|          C2S1|\n",
      "|[0.47382780245281...|  0.0|       0.0|          C3S1|\n",
      "|[0.47382780245281...|  1.0|       1.0|          C2S1|\n",
      "|[0.48174695417526...|  1.0|       1.0|          C2S1|\n",
      "|[0.48306680755103...|  0.0|       0.0|          C3S1|\n",
      "|[0.48834622105414...|  0.0|       0.0|          C3S1|\n",
      "|[0.52134258691634...|  0.0|       0.0|          C3S1|\n",
      "|[0.55829851290588...|  0.0|       0.0|          C3S1|\n",
      "|[0.58601546526497...|  0.0|       0.0|          C3S1|\n",
      "|[0.59921399902274...|  3.0|       3.0|          C4S2|\n",
      "|[0.72460016412487...|  0.0|       0.0|          C3S1|\n",
      "|[0.76683553508530...|  1.0|       1.0|          C2S1|\n",
      "|[0.80247160769906...|  0.0|       0.0|          C3S1|\n",
      "+--------------------+-----+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Train a RandomForestClassifier\n",
    "rf = RandomForestClassifier(featuresCol='features', labelCol='label', numTrees=100)\n",
    "rf_model = rf.fit(train_data)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "rf_predictions = rf_model.transform(val_data)\n",
    "\n",
    "# Evaluate the model\n",
    "rf_evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='accuracy')\n",
    "accuracy = rf_evaluator.evaluate(rf_predictions)\n",
    "\n",
    "print(f\"Validation Accuracy: {accuracy}\")\n",
    "\n",
    "# Show some predictions\n",
    "rf_predictions.select(\"features\", \"label\", \"prediction\",\"Classification\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9408284023668639\n",
      "+--------------------+-----+----------+--------------+\n",
      "|            features|label|prediction|Classification|\n",
      "+--------------------+-----+----------+--------------+\n",
      "|[0.13858473032769...|  0.0|       0.0|          C3S1|\n",
      "|[0.21777601154375...|  0.0|       0.0|          C3S1|\n",
      "|[0.23229441441119...|  1.0|       1.0|          C2S1|\n",
      "|[0.27980916740693...|  0.0|       0.0|          C3S1|\n",
      "|[0.33260336537357...|  1.0|       1.0|          C2S1|\n",
      "|[0.34316219237978...|  0.0|       0.0|          C3S1|\n",
      "|[0.36296002448422...|  0.0|       2.0|          C3S1|\n",
      "|[0.40255565722530...|  1.0|       9.0|          C2S1|\n",
      "|[0.47382780245281...|  0.0|       0.0|          C3S1|\n",
      "|[0.47382780245281...|  1.0|       1.0|          C2S1|\n",
      "|[0.48174695417526...|  1.0|       1.0|          C2S1|\n",
      "|[0.48306680755103...|  0.0|       0.0|          C3S1|\n",
      "|[0.48834622105414...|  0.0|       0.0|          C3S1|\n",
      "|[0.52134258691634...|  0.0|       0.0|          C3S1|\n",
      "|[0.55829851290588...|  0.0|       0.0|          C3S1|\n",
      "|[0.58601546526497...|  0.0|       0.0|          C3S1|\n",
      "|[0.59921399902274...|  3.0|       3.0|          C4S2|\n",
      "|[0.72460016412487...|  0.0|       0.0|          C3S1|\n",
      "|[0.76683553508530...|  1.0|       1.0|          C2S1|\n",
      "|[0.80247160769906...|  0.0|       0.0|          C3S1|\n",
      "+--------------------+-----+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "dt = DecisionTreeClassifier(featuresCol='features', labelCol='label')\n",
    "dt_model = dt.fit(train_data)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "dt_predictions = dt_model.transform(val_data)\n",
    "\n",
    "# Evaluate the model\n",
    "dt_evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='accuracy')\n",
    "dt_accuracy = dt_evaluator.evaluate(dt_predictions)\n",
    "\n",
    "print(f\"Validation Accuracy: {dt_accuracy}\")\n",
    "\n",
    "# Show some predictions\n",
    "dt_predictions.select(\"features\", \"label\", \"prediction\",\"Classification\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
